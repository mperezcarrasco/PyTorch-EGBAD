{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import EGBADTrainer\n",
    "from preprocess import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    num_epochs=100\n",
    "    lr=1e-4\n",
    "    latent_dim=256\n",
    "    anormal_class=1\n",
    "    batch_size=128\n",
    "    pretrained=False\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_mnist(args)\n",
    "\n",
    "egbad = EGBADTrainer(args, data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000: [===============================>] - ETA 0.4sss\n",
      "Training... Epoch: 0, Discrimiantor Loss: 0.031, Generator Loss: 17.806\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 1, Discrimiantor Loss: 0.001, Generator Loss: 26.728\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 2, Discrimiantor Loss: 0.001, Generator Loss: 30.853\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 3, Discrimiantor Loss: 0.000, Generator Loss: 34.406\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 4, Discrimiantor Loss: 0.001, Generator Loss: 36.387\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 5, Discrimiantor Loss: 0.001, Generator Loss: 40.797\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 6, Discrimiantor Loss: 0.001, Generator Loss: 42.852\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 7, Discrimiantor Loss: 0.001, Generator Loss: 45.509\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 8, Discrimiantor Loss: 0.001, Generator Loss: 46.640\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 9, Discrimiantor Loss: 0.001, Generator Loss: 48.777\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 10, Discrimiantor Loss: 0.001, Generator Loss: 48.839\n",
      "60000/60000: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 11, Discrimiantor Loss: 0.001, Generator Loss: 50.213\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 12, Discrimiantor Loss: 0.001, Generator Loss: 50.550\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 13, Discrimiantor Loss: 0.001, Generator Loss: 51.610\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 14, Discrimiantor Loss: 0.001, Generator Loss: 51.116\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 15, Discrimiantor Loss: 0.002, Generator Loss: 54.627\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 16, Discrimiantor Loss: 0.006, Generator Loss: 49.342\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 17, Discrimiantor Loss: 0.002, Generator Loss: 49.421\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 18, Discrimiantor Loss: 0.007, Generator Loss: 51.068\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 19, Discrimiantor Loss: 0.008, Generator Loss: 48.739\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 20, Discrimiantor Loss: 0.006, Generator Loss: 46.949\n",
      "60000/60000: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 21, Discrimiantor Loss: 0.010, Generator Loss: 46.846\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 22, Discrimiantor Loss: 0.009, Generator Loss: 45.168\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 23, Discrimiantor Loss: 0.008, Generator Loss: 43.260\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 24, Discrimiantor Loss: 0.007, Generator Loss: 43.352\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 25, Discrimiantor Loss: 0.012, Generator Loss: 41.526\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 26, Discrimiantor Loss: 0.011, Generator Loss: 39.903\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 27, Discrimiantor Loss: 0.012, Generator Loss: 37.715\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 28, Discrimiantor Loss: 0.020, Generator Loss: 35.808\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 29, Discrimiantor Loss: 0.014, Generator Loss: 33.998\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 30, Discrimiantor Loss: 0.017, Generator Loss: 34.665\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 31, Discrimiantor Loss: 0.021, Generator Loss: 32.832\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 32, Discrimiantor Loss: 0.025, Generator Loss: 30.711\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 33, Discrimiantor Loss: 0.031, Generator Loss: 29.637\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 34, Discrimiantor Loss: 0.029, Generator Loss: 27.973\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 35, Discrimiantor Loss: 0.026, Generator Loss: 28.667\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 36, Discrimiantor Loss: 0.034, Generator Loss: 27.158\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 37, Discrimiantor Loss: 0.033, Generator Loss: 27.639\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 38, Discrimiantor Loss: 0.048, Generator Loss: 25.602\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 39, Discrimiantor Loss: 0.043, Generator Loss: 24.796\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 40, Discrimiantor Loss: 0.075, Generator Loss: 22.326\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 41, Discrimiantor Loss: 0.059, Generator Loss: 21.017\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 42, Discrimiantor Loss: 0.062, Generator Loss: 21.716\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 43, Discrimiantor Loss: 0.069, Generator Loss: 21.578\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 44, Discrimiantor Loss: 0.074, Generator Loss: 20.535\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 45, Discrimiantor Loss: 0.101, Generator Loss: 18.698\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 46, Discrimiantor Loss: 0.094, Generator Loss: 18.357\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 47, Discrimiantor Loss: 0.100, Generator Loss: 18.163\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 48, Discrimiantor Loss: 0.115, Generator Loss: 17.271\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 49, Discrimiantor Loss: 0.116, Generator Loss: 16.664\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 50, Discrimiantor Loss: 0.118, Generator Loss: 16.926\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 51, Discrimiantor Loss: 0.130, Generator Loss: 16.299\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 52, Discrimiantor Loss: 0.159, Generator Loss: 15.347\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 53, Discrimiantor Loss: 0.165, Generator Loss: 14.854\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 54, Discrimiantor Loss: 0.159, Generator Loss: 14.981\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 55, Discrimiantor Loss: 0.180, Generator Loss: 14.295\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 56, Discrimiantor Loss: 0.177, Generator Loss: 14.029\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 57, Discrimiantor Loss: 0.215, Generator Loss: 13.240\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 58, Discrimiantor Loss: 0.225, Generator Loss: 12.643\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 59, Discrimiantor Loss: 0.236, Generator Loss: 12.457\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 60, Discrimiantor Loss: 0.238, Generator Loss: 11.938\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 61, Discrimiantor Loss: 0.286, Generator Loss: 11.213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 62, Discrimiantor Loss: 0.279, Generator Loss: 11.311\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 63, Discrimiantor Loss: 0.290, Generator Loss: 10.808\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 64, Discrimiantor Loss: 0.330, Generator Loss: 10.445\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 65, Discrimiantor Loss: 0.311, Generator Loss: 10.226\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 66, Discrimiantor Loss: 0.337, Generator Loss: 9.878\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 67, Discrimiantor Loss: 0.355, Generator Loss: 9.495\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 68, Discrimiantor Loss: 0.380, Generator Loss: 9.198\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 69, Discrimiantor Loss: 0.391, Generator Loss: 9.021\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 70, Discrimiantor Loss: 0.403, Generator Loss: 8.725\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 71, Discrimiantor Loss: 0.424, Generator Loss: 8.542\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 72, Discrimiantor Loss: 0.458, Generator Loss: 8.088\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 73, Discrimiantor Loss: 0.492, Generator Loss: 7.497\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 74, Discrimiantor Loss: 0.476, Generator Loss: 7.568\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 75, Discrimiantor Loss: 0.508, Generator Loss: 7.184\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 76, Discrimiantor Loss: 0.539, Generator Loss: 6.938\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 77, Discrimiantor Loss: 0.534, Generator Loss: 6.887\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 78, Discrimiantor Loss: 0.550, Generator Loss: 6.680\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 79, Discrimiantor Loss: 0.574, Generator Loss: 6.366\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 80, Discrimiantor Loss: 0.611, Generator Loss: 6.169\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 81, Discrimiantor Loss: 0.595, Generator Loss: 6.155\n",
      "60000/60000: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 82, Discrimiantor Loss: 0.616, Generator Loss: 6.053\n",
      " 7552/60000: [===>............................] - ETA 18.9s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ef5690e75436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0megbad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PyTorch-EGBAD/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mge_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0md_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0;31m#Defining labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/barbar/Bar.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PyTorch-EGBAD/preprocess.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;31m# PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "egbad.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "alpha = 0.9\n",
    "labels = []\n",
    "scores = []\n",
    "\n",
    "egbad.G.eval()\n",
    "egbad.D.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, label in data[1]:\n",
    "        img = img.float().to(device)\n",
    "        img_hat = egbad.G(egbad.E(img))\n",
    "        score_g = torch.sum(torch.abs(img - img_hat), dim=(1,2,3))\n",
    "        \n",
    "        y_true = Variable(torch.ones((img.size(0), 1)).to(device))\n",
    "        y_pred = egbad.D(img, egbad.E(img)).view(-1)\n",
    "        score_d = F.binary_cross_entropy(y_pred, y_true)\n",
    "        \n",
    "        score = alpha * score_g + (1-alpha)* score_d\n",
    "        scores.append(score.cpu())\n",
    "        labels.append(label.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cat(scores, dim=0)\n",
    "labels = torch.cat(labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "\n",
    "print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels, scores)*100))\n",
    "print('PR AUC score: {:.2f}'.format(auc(recall, precision)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(scores[labels==0], color='r', alpha = 0.3)\n",
    "plt.hist(scores[labels==1], color='b', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
