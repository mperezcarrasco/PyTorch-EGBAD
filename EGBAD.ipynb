{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import TrainerEGBAD\n",
    "from preprocess import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    num_epochs=100\n",
    "    lr=0.00001\n",
    "    latent_dim=200\n",
    "    anormal_class=0\n",
    "    batch_size=100\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_mnist(args)\n",
    "\n",
    "egbad = TrainerEGBAD(args, data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43261/43261: [===============================>] - ETA 0.4sss\n",
      "Training... Epoch: 0, Discrimiantor Loss: 1.181, Generator Loss: 1.665\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 1, Discrimiantor Loss: 0.285, Generator Loss: 4.903\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 2, Discrimiantor Loss: 0.046, Generator Loss: 8.626\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 3, Discrimiantor Loss: 0.017, Generator Loss: 10.875\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 4, Discrimiantor Loss: 0.010, Generator Loss: 12.255\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 5, Discrimiantor Loss: 0.006, Generator Loss: 13.375\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 6, Discrimiantor Loss: 0.004, Generator Loss: 14.454\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 7, Discrimiantor Loss: 0.003, Generator Loss: 15.347\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 8, Discrimiantor Loss: 0.004, Generator Loss: 15.884\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 9, Discrimiantor Loss: 0.002, Generator Loss: 17.767\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 10, Discrimiantor Loss: 0.001, Generator Loss: 17.975\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 11, Discrimiantor Loss: 0.001, Generator Loss: 18.613\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 12, Discrimiantor Loss: 0.001, Generator Loss: 18.864\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 13, Discrimiantor Loss: 0.001, Generator Loss: 19.978\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 14, Discrimiantor Loss: 0.000, Generator Loss: 20.895\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 15, Discrimiantor Loss: 0.000, Generator Loss: 21.687\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 16, Discrimiantor Loss: 0.000, Generator Loss: 22.442\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 17, Discrimiantor Loss: 0.000, Generator Loss: 23.192\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 18, Discrimiantor Loss: 0.005, Generator Loss: 26.990\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 19, Discrimiantor Loss: 0.000, Generator Loss: 32.077\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 20, Discrimiantor Loss: 0.000, Generator Loss: 28.069\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 21, Discrimiantor Loss: 0.000, Generator Loss: 27.489\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 22, Discrimiantor Loss: 0.000, Generator Loss: 27.445\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 23, Discrimiantor Loss: 0.000, Generator Loss: 27.389\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 24, Discrimiantor Loss: 0.000, Generator Loss: 27.376\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 25, Discrimiantor Loss: 0.000, Generator Loss: 27.495\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 26, Discrimiantor Loss: 0.000, Generator Loss: 27.832\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 27, Discrimiantor Loss: 0.000, Generator Loss: 28.089\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 28, Discrimiantor Loss: 0.000, Generator Loss: 28.447\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 29, Discrimiantor Loss: 0.000, Generator Loss: 28.808\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 30, Discrimiantor Loss: 0.000, Generator Loss: 29.082\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 31, Discrimiantor Loss: 0.000, Generator Loss: 29.644\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 32, Discrimiantor Loss: 0.000, Generator Loss: 30.066\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 33, Discrimiantor Loss: 0.000, Generator Loss: 30.671\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 34, Discrimiantor Loss: 0.000, Generator Loss: 31.454\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 35, Discrimiantor Loss: 0.000, Generator Loss: 31.976\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 36, Discrimiantor Loss: 0.000, Generator Loss: 32.376\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 37, Discrimiantor Loss: 0.003, Generator Loss: 35.407\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 38, Discrimiantor Loss: 0.000, Generator Loss: 34.113\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 39, Discrimiantor Loss: 0.000, Generator Loss: 32.835\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 40, Discrimiantor Loss: 0.000, Generator Loss: 32.140\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 41, Discrimiantor Loss: 0.000, Generator Loss: 32.101\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 42, Discrimiantor Loss: 0.000, Generator Loss: 31.368\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 43, Discrimiantor Loss: 0.000, Generator Loss: 30.969\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 44, Discrimiantor Loss: 0.000, Generator Loss: 30.801\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 45, Discrimiantor Loss: 0.001, Generator Loss: 30.924\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 46, Discrimiantor Loss: 0.001, Generator Loss: 30.861\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 47, Discrimiantor Loss: 0.000, Generator Loss: 30.657\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 48, Discrimiantor Loss: 0.000, Generator Loss: 30.953\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 49, Discrimiantor Loss: 0.000, Generator Loss: 31.221\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 50, Discrimiantor Loss: 0.000, Generator Loss: 31.319\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 51, Discrimiantor Loss: 0.001, Generator Loss: 31.305\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 52, Discrimiantor Loss: 0.000, Generator Loss: 31.608\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 53, Discrimiantor Loss: 0.001, Generator Loss: 31.528\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 54, Discrimiantor Loss: 0.001, Generator Loss: 31.870\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 55, Discrimiantor Loss: 0.000, Generator Loss: 31.999\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 56, Discrimiantor Loss: 0.002, Generator Loss: 33.453\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 57, Discrimiantor Loss: 0.001, Generator Loss: 32.047\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 58, Discrimiantor Loss: 0.001, Generator Loss: 31.759\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 59, Discrimiantor Loss: 0.001, Generator Loss: 30.745\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 60, Discrimiantor Loss: 0.001, Generator Loss: 30.690\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 61, Discrimiantor Loss: 0.001, Generator Loss: 30.609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 62, Discrimiantor Loss: 0.001, Generator Loss: 30.215\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 63, Discrimiantor Loss: 0.002, Generator Loss: 30.163\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 64, Discrimiantor Loss: 0.001, Generator Loss: 30.332\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 65, Discrimiantor Loss: 0.001, Generator Loss: 30.366\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 66, Discrimiantor Loss: 0.003, Generator Loss: 31.260\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 67, Discrimiantor Loss: 0.002, Generator Loss: 31.176\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 68, Discrimiantor Loss: 0.001, Generator Loss: 30.630\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 69, Discrimiantor Loss: 0.001, Generator Loss: 30.045\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 70, Discrimiantor Loss: 0.001, Generator Loss: 30.228\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 71, Discrimiantor Loss: 0.002, Generator Loss: 30.222\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 72, Discrimiantor Loss: 0.001, Generator Loss: 30.155\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 73, Discrimiantor Loss: 0.002, Generator Loss: 29.304\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 74, Discrimiantor Loss: 0.003, Generator Loss: 28.758\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 75, Discrimiantor Loss: 0.003, Generator Loss: 27.572\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 76, Discrimiantor Loss: 0.005, Generator Loss: 27.677\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 77, Discrimiantor Loss: 0.004, Generator Loss: 26.105\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 78, Discrimiantor Loss: 0.005, Generator Loss: 25.072\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 79, Discrimiantor Loss: 0.005, Generator Loss: 25.120\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 80, Discrimiantor Loss: 0.006, Generator Loss: 24.251\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 81, Discrimiantor Loss: 0.005, Generator Loss: 25.016\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 82, Discrimiantor Loss: 0.005, Generator Loss: 25.587\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 83, Discrimiantor Loss: 0.005, Generator Loss: 25.520\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 84, Discrimiantor Loss: 0.011, Generator Loss: 24.657\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 85, Discrimiantor Loss: 0.028, Generator Loss: 23.474\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 86, Discrimiantor Loss: 0.018, Generator Loss: 23.190\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 87, Discrimiantor Loss: 0.018, Generator Loss: 22.634\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 88, Discrimiantor Loss: 0.018, Generator Loss: 21.672\n",
      "43261/43261: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 89, Discrimiantor Loss: 0.017, Generator Loss: 20.918\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 90, Discrimiantor Loss: 0.012, Generator Loss: 22.283\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 91, Discrimiantor Loss: 0.017, Generator Loss: 22.096\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 92, Discrimiantor Loss: 0.016, Generator Loss: 22.641\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 93, Discrimiantor Loss: 0.016, Generator Loss: 23.063\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 94, Discrimiantor Loss: 0.013, Generator Loss: 24.383\n",
      "43261/43261: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 95, Discrimiantor Loss: 0.014, Generator Loss: 23.783\n",
      " 3200/43261: [==>.............................] - ETA 19.4s"
     ]
    }
   ],
   "source": [
    "egbad.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "alpha = 0.9\n",
    "labels = []\n",
    "scores = []\n",
    "\n",
    "egbad.G.eval()\n",
    "egbad.D.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, label in data[1]:\n",
    "        img = img.float().to(device)\n",
    "        img_hat = egbad.G(egbad.E(img))\n",
    "        score_g = torch.sum(torch.abs(img - img_hat), dim=(1,2,3))\n",
    "        \n",
    "        y_true = Variable(torch.ones((img.size(0), 1)).to(device))\n",
    "        y_pred = egbad.D(img, egbad.E(img)).view(-1)\n",
    "        score_d = F.binary_cross_entropy(y_pred, y_true)\n",
    "        \n",
    "        score = alpha * score_g + (1-alpha)* score_d\n",
    "        scores.append(score.cpu())\n",
    "        labels.append(label.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cat(scores, dim=0)\n",
    "labels = torch.cat(labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "\n",
    "print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels, scores)*100))\n",
    "print('PR AUC score: {:.2f}'.format(auc(recall, precision)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(scores[labels==0], color='r')\n",
    "plt.hist(scores[labels==1], color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
