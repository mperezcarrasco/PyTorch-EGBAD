{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import EGBADTrainer\n",
    "from preprocess import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    num_epochs=100\n",
    "    lr=1e-4\n",
    "    latent_dim=256\n",
    "    anormal_class=1\n",
    "    batch_size=128\n",
    "    pretrained=False\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_mnist(args)\n",
    "\n",
    "egbad = EGBADTrainer(args, data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42606/42606: [===============================>] - ETA 0.3sss\n",
      "Training... Epoch: 0, Discrimiantor Loss: 0.158, Generator Loss: 13.776\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 1, Discrimiantor Loss: 0.234, Generator Loss: 11.356\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 2, Discrimiantor Loss: 0.287, Generator Loss: 10.251\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 3, Discrimiantor Loss: 0.486, Generator Loss: 6.838\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 4, Discrimiantor Loss: 0.693, Generator Loss: 4.730\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 5, Discrimiantor Loss: 0.672, Generator Loss: 4.954\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 6, Discrimiantor Loss: 0.744, Generator Loss: 4.419\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 7, Discrimiantor Loss: 0.800, Generator Loss: 4.206\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 8, Discrimiantor Loss: 0.811, Generator Loss: 4.088\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 9, Discrimiantor Loss: 0.865, Generator Loss: 3.804\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 10, Discrimiantor Loss: 0.895, Generator Loss: 3.730\n",
      "42606/42606: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 11, Discrimiantor Loss: 0.889, Generator Loss: 3.696\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 12, Discrimiantor Loss: 0.919, Generator Loss: 3.567\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 13, Discrimiantor Loss: 0.913, Generator Loss: 3.551\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 14, Discrimiantor Loss: 0.956, Generator Loss: 3.400\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 15, Discrimiantor Loss: 0.964, Generator Loss: 3.345\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 16, Discrimiantor Loss: 0.957, Generator Loss: 3.365\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 17, Discrimiantor Loss: 0.945, Generator Loss: 3.377\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 18, Discrimiantor Loss: 0.968, Generator Loss: 3.345\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 19, Discrimiantor Loss: 0.967, Generator Loss: 3.338\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 20, Discrimiantor Loss: 0.967, Generator Loss: 3.353\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 21, Discrimiantor Loss: 0.974, Generator Loss: 3.351\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 22, Discrimiantor Loss: 0.992, Generator Loss: 3.303\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 23, Discrimiantor Loss: 0.973, Generator Loss: 3.324\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 24, Discrimiantor Loss: 0.992, Generator Loss: 3.306\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 25, Discrimiantor Loss: 0.991, Generator Loss: 3.317\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 26, Discrimiantor Loss: 0.986, Generator Loss: 3.350\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 27, Discrimiantor Loss: 0.984, Generator Loss: 3.391\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 28, Discrimiantor Loss: 1.001, Generator Loss: 3.350\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 29, Discrimiantor Loss: 0.994, Generator Loss: 3.372\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 30, Discrimiantor Loss: 1.003, Generator Loss: 3.375\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 31, Discrimiantor Loss: 1.002, Generator Loss: 3.368\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 32, Discrimiantor Loss: 0.998, Generator Loss: 3.378\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 33, Discrimiantor Loss: 0.997, Generator Loss: 3.427\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 34, Discrimiantor Loss: 1.002, Generator Loss: 3.425\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 35, Discrimiantor Loss: 1.005, Generator Loss: 3.422\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 36, Discrimiantor Loss: 0.983, Generator Loss: 3.507\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 37, Discrimiantor Loss: 0.997, Generator Loss: 3.486\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 38, Discrimiantor Loss: 1.008, Generator Loss: 3.415\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 39, Discrimiantor Loss: 1.000, Generator Loss: 3.423\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 40, Discrimiantor Loss: 1.001, Generator Loss: 3.425\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 41, Discrimiantor Loss: 1.006, Generator Loss: 3.424\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 42, Discrimiantor Loss: 1.013, Generator Loss: 3.392\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 43, Discrimiantor Loss: 0.991, Generator Loss: 3.487\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 44, Discrimiantor Loss: 0.998, Generator Loss: 3.473\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 45, Discrimiantor Loss: 1.013, Generator Loss: 3.437\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 46, Discrimiantor Loss: 0.997, Generator Loss: 3.446\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 47, Discrimiantor Loss: 0.991, Generator Loss: 3.468\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 48, Discrimiantor Loss: 1.009, Generator Loss: 3.434\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 49, Discrimiantor Loss: 0.970, Generator Loss: 3.545\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 50, Discrimiantor Loss: 1.002, Generator Loss: 3.453\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 51, Discrimiantor Loss: 0.979, Generator Loss: 3.462\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 52, Discrimiantor Loss: 1.001, Generator Loss: 3.486\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 53, Discrimiantor Loss: 0.995, Generator Loss: 3.475\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 54, Discrimiantor Loss: 0.997, Generator Loss: 3.493\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 55, Discrimiantor Loss: 0.987, Generator Loss: 3.514\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 56, Discrimiantor Loss: 0.994, Generator Loss: 3.479\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 57, Discrimiantor Loss: 0.991, Generator Loss: 3.508\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 58, Discrimiantor Loss: 0.981, Generator Loss: 3.531\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 59, Discrimiantor Loss: 0.991, Generator Loss: 3.515\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 60, Discrimiantor Loss: 1.003, Generator Loss: 3.466\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 61, Discrimiantor Loss: 0.970, Generator Loss: 3.538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 62, Discrimiantor Loss: 0.996, Generator Loss: 3.458\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 63, Discrimiantor Loss: 1.002, Generator Loss: 3.424\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 64, Discrimiantor Loss: 0.983, Generator Loss: 3.505\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 65, Discrimiantor Loss: 0.995, Generator Loss: 3.442\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 66, Discrimiantor Loss: 0.988, Generator Loss: 3.465\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 67, Discrimiantor Loss: 0.970, Generator Loss: 3.522\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 68, Discrimiantor Loss: 0.988, Generator Loss: 3.471\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 69, Discrimiantor Loss: 0.978, Generator Loss: 3.498\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 70, Discrimiantor Loss: 0.976, Generator Loss: 3.492\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 71, Discrimiantor Loss: 0.977, Generator Loss: 3.505\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 72, Discrimiantor Loss: 0.988, Generator Loss: 3.467\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 73, Discrimiantor Loss: 0.972, Generator Loss: 3.498\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 74, Discrimiantor Loss: 0.999, Generator Loss: 3.447\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 75, Discrimiantor Loss: 0.976, Generator Loss: 3.503\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 76, Discrimiantor Loss: 0.976, Generator Loss: 3.509\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 77, Discrimiantor Loss: 0.957, Generator Loss: 3.521\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 78, Discrimiantor Loss: 0.976, Generator Loss: 3.493\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 79, Discrimiantor Loss: 0.979, Generator Loss: 3.465\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 80, Discrimiantor Loss: 0.969, Generator Loss: 3.483\n",
      "42606/42606: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 81, Discrimiantor Loss: 0.974, Generator Loss: 3.446\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 82, Discrimiantor Loss: 0.986, Generator Loss: 3.483\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 83, Discrimiantor Loss: 0.971, Generator Loss: 3.429\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 84, Discrimiantor Loss: 0.961, Generator Loss: 3.490\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 85, Discrimiantor Loss: 0.973, Generator Loss: 3.498\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 86, Discrimiantor Loss: 0.989, Generator Loss: 3.433\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 87, Discrimiantor Loss: 0.971, Generator Loss: 3.434\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 88, Discrimiantor Loss: 0.981, Generator Loss: 3.453\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 89, Discrimiantor Loss: 0.970, Generator Loss: 3.447\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 90, Discrimiantor Loss: 0.979, Generator Loss: 3.442\n",
      "42606/42606: [===============================>] - ETA 0.1ss\n",
      "Training... Epoch: 91, Discrimiantor Loss: 0.969, Generator Loss: 3.456\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 92, Discrimiantor Loss: 0.970, Generator Loss: 3.432\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 93, Discrimiantor Loss: 0.984, Generator Loss: 3.430\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 94, Discrimiantor Loss: 0.975, Generator Loss: 3.407\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 95, Discrimiantor Loss: 0.968, Generator Loss: 3.414\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 96, Discrimiantor Loss: 0.967, Generator Loss: 3.501\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 97, Discrimiantor Loss: 0.954, Generator Loss: 3.528\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 98, Discrimiantor Loss: 0.965, Generator Loss: 3.489\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 99, Discrimiantor Loss: 0.961, Generator Loss: 3.490\n",
      "42606/42606: [===============================>] - ETA 0.0ss\n",
      "Training... Epoch: 100, Discrimiantor Loss: 0.984, Generator Loss: 3.391\n"
     ]
    }
   ],
   "source": [
    "egbad.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])) is deprecated. Please ensure they have the same size.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Using a target size (torch.Size([114, 1])) that is different to the input size (torch.Size([114])) is deprecated. Please ensure they have the same size.\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "alpha = 0.9\n",
    "labels = []\n",
    "scores = []\n",
    "\n",
    "egbad.G.eval()\n",
    "egbad.D.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, label in data[1]:\n",
    "        img = img.float().to(device)\n",
    "        img_hat = egbad.G(egbad.E(img))\n",
    "        score_g = torch.sum(torch.abs(img - img_hat), dim=(1,2,3))\n",
    "        \n",
    "        y_true = Variable(torch.ones((img.size(0), 1)).to(device))\n",
    "        y_pred = egbad.D(img, egbad.E(img)).view(-1)\n",
    "        score_d = F.binary_cross_entropy(y_pred, y_true)\n",
    "        \n",
    "        score = alpha * score_g + (1-alpha)* score_d\n",
    "        scores.append(score.cpu())\n",
    "        labels.append(label.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cat(scores, dim=0)\n",
    "labels = torch.cat(labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 38.86\n",
      "PR AUC score: 30.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "\n",
    "print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels, scores)*100))\n",
    "print('PR AUC score: {:.2f}'.format(auc(recall, precision)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(scores[labels==0], color='r', alpha = 0.3)\n",
    "plt.hist(scores[labels==1], color='b', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
